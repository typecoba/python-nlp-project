{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "인스타그램의 댓글 데이터셋을 가지고 자연어처리를 통해 분석해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.normalizer import *\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "fpath = '../data/comment/md_comment_삼분의일.txt' # 특정범위 comment\n",
    "fpath_train = '../data/comment/cel_comment_all.txt' # comment 전체 데이터\n",
    "fpath_train_text_only = '../data/comment/cel_comment_all_text_only.txt' # 이모지 제거된 text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = DoublespaceLineCorpus(fpath_train, iter_sent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5149216\n",
      "['만듀',\n",
      " '야시기 아치미다',\n",
      " '물만두 완전 빠인데ᆢ',\n",
      " '야침 물만두',\n",
      " 'That heart shaped dish is cute ????',\n",
      " '물만두는 라면에 넣어도 맛있어요 ㅎㅎ',\n",
      " '모닝 만두인까요?',\n",
      " '????',\n",
      " '????????',\n",
      " '앙증맞은 물만두♥.♥ 물만두 왜케 귀엽죵?ㅋㅋㅋㅋㅋ 팔로우 하고 갑니다♡ 자주',\n",
      " '소통해요! 소통스타그램????',\n",
      " '만듀만듀만듀만듀만듀~',\n",
      " '꿀맛 만듀만듀~ㅋ',\n",
      " '나 먼저 출근 멘붕',\n",
      " '우왕 맛나것다 맨듀',\n",
      " '으억맛있겠다????????',\n",
      " '옷 진짜 잘입으시네요~~패션감각 부러워용',\n",
      " '목걸이 판매하시나요?',\n",
      " '목걸이 너무 예뻐요✨',\n",
      " '늘 밸런스가 최고이심 ????',\n",
      " '뭐라도 자꾸 사고싶다능 ????',\n",
      " '개미지옥 에스실',\n",
      " '자꾸 뭐라도사야할거같다는????????????',\n",
      " '목걸이 현재도 구매가능할런지요??',\n",
      " '사진 잘보고 가요 :D????',\n",
      " '@wnwncjh 감사합니다^^목걸이는 저희 #에스실 제품이에요^^ 웹사이트에서 구매하실수 있어요:)',\n",
      " '여신????',\n",
      " '여신…♥♥♥',\n",
      " '언니다아~~~>_<!!',\n",
      " '@rainbowoori 꺄. 우리다 ☺️',\n",
      " '히히히!!! 왔오 왔오♡♡♡',\n",
      " '안녕하세요 맞팔해용',\n",
      " '바지 정보 알수있을까요????',\n",
      " '오늘한주가 시작된만큼..좋은하루보내세요~^^',\n",
      " '이뻐요',\n",
      " '저도 나이들면 꼭 보육원을하고싶네요!!천사아이들을 맛있는거많이해주구요~^^*',\n",
      " '@shimjinhwa 카앙❤️❤️❤️❤️❤️',\n",
      " '미녀는 잠꾸러기~~^^*',\n",
      " '어쩜 자는것도 이뻐염????',\n",
      " '자는 모습에 천사네요....???? 즐거운 명절 보네세요...???? 감기도 조심하시구요..????????????',\n",
      " '잠자는 숲속의 공주같닷♡',\n",
      " '어~♥♥♥심♥쿵…',\n",
      " '언니 진짜 이뻐요ㅜㅜ♡',\n",
      " '썬배님 해피 추석 보내세요????',\n",
      " '위ㅟ대한조강지처보고잇어요 즐추~♡~♡',\n",
      " '즐거운 추석보내♡',\n",
      " '풍성한 한가위보내세요~~^^ 기름진음식은 적당히..!!',\n",
      " '@ainii 헤헤 감사합니다^^ 굿밤되세요????',\n",
      " '@greyisnewblack 어머! 멋쟁이신 그레이님 감사해요 꾸벅????히히 예쁜딸래미랑 행복한밤되세요????',\n",
      " '@flyhyunjung 제가 구매욕을,,,근데 실물이 더 예뻐요(소곤소곤????)']\n"
     ]
    }
   ],
   "source": [
    "# corpus 확인.. 이모지가 보이네...\n",
    "print(len(list(corpus)))\n",
    "pprint(list(corpus)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @태그 #을 없애고 이모지 제거..\n",
    "sents = []\n",
    "for i, sent in enumerate(corpus):\n",
    "    sent = re.sub(r'(@[^\\s]+\\s)|#', '', sent) # 정규식으로 @___, # 제거\n",
    "    sent = re.sub('\\?*','',sent) # ???? 제거\n",
    "    sent = repeat_normalize(only_text(sent)) # 이모지제거, 반복문자 줄임\n",
    "    sents.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5149216\n",
      "['만듀',\n",
      " '야시기 아치미다',\n",
      " '물만두 완전 빠인데',\n",
      " '야침 물만두',\n",
      " 'That heart shaped dish is cute',\n",
      " '물만두는 라면에 넣어도 맛있어요 ㅎㅎ',\n",
      " '모닝 만두인까요',\n",
      " '',\n",
      " '',\n",
      " '앙증맞은 물만두 . 물만두 왜케 귀엽죵ㅋㅋ 팔로우 하고 갑니다 자주',\n",
      " '소통해요! 소통스타그램',\n",
      " '만듀만듀만듀만듀만듀',\n",
      " '꿀맛 만듀만듀 ㅋ',\n",
      " '나 먼저 출근 멘붕',\n",
      " '우왕 맛나것다 맨듀',\n",
      " '으억맛있겠다',\n",
      " '옷 진짜 잘입으시네요 패션감각 부러워용',\n",
      " '목걸이 판매하시나요',\n",
      " '목걸이 너무 예뻐요',\n",
      " '늘 밸런스가 최고이심',\n",
      " '뭐라도 자꾸 사고싶다능',\n",
      " '개미지옥 에스실',\n",
      " '자꾸 뭐라도사야할거같다는',\n",
      " '목걸이 현재도 구매가능할런지요',\n",
      " '사진 잘보고 가요 D',\n",
      " '감사합니다 목걸이는 저희 에스실 제품이에요 웹사이트에서 구매하실수 있어요 )',\n",
      " '여신',\n",
      " '여신',\n",
      " '언니다아 !!',\n",
      " '꺄. 우리다']\n"
     ]
    }
   ],
   "source": [
    "print(len(sents))\n",
    "pprint(sents[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "만듀\n",
      "야시기 아치미다\n",
      "물만두 완전 빠인데\n",
      "야침 물만두\n",
      "That heart shaped dish is cute \n",
      "물만두는 라면에 넣어도 맛있어요 ㅎㅎ\n",
      "모닝 만두인까요\n",
      "\n",
      "\n",
      "앙증맞은 물만두 . 물만두 왜케 귀엽죵ㅋㅋ 팔로우 하고 갑니다 자주\n",
      "소통해요! 소통스타그램\n"
     ]
    }
   ],
   "source": [
    "for i,sent in enumerate(sents) : \n",
    "    if i > 10 : break\n",
    "    print(re.sub('\\?*','',sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이모지제거 후 저장파일\n",
    "temp = ''\n",
    "for sent in sents : \n",
    "    if sent!='':\n",
    "        temp += sent+'  '\n",
    "\n",
    "with open(fpath_train_text_only, 'w', encoding='utf-8') as f :\n",
    "    f.write(temp)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\mediance_ssh\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result)\u001b[0m\n\u001b[0;32m   2877\u001b[0m                 \u001b[1;31m#rprint('Running code', repr(code_obj)) # dbg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2878\u001b[1;33m                 \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2879\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-c99a5daccc2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountSpace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpath_train_text_only\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\mediance_ssh\\Anaconda3\\lib\\site-packages\\soyspacing\\countbase\\_countbase.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, fname, num_lines)\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_window\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_window\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mchars_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mediance_ssh\\Anaconda3\\lib\\site-packages\\soyspacing\\countbase\\_countbase.py\u001b[0m in \u001b[0;36m_extract\u001b[1;34m(self, chars, tags, window)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mediance_ssh\\Anaconda3\\lib\\site-packages\\soyspacing\\countbase\\_countbase.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# spacing\n",
    "# 더 큰 데이터셋으로 학습시켜야 함 -> 이모지 제거하니까 해결...\n",
    "# 그런데... 학습결과가 좋지않았다.. 학습된 내용 자체가 문제가 있으니까... 아무래도\n",
    "# 정답을 학습시켜야 하는데.. 그럼 대량의 다양한 카테고리의 뉴스데이터를 학습시켜야 함\n",
    "from soyspacing.countbase import RuleDict, CountSpace\n",
    "# print(soyspacing.__version__)\n",
    "\n",
    "model = CountSpace()\n",
    "model.train(fpath_train_text_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존매트리스오래되어서 버리고 설치했어요. 프레임에 딱 맞게 들어가구요 타입은 약간폭신 과 하드한 타입이있는데 구매평에 다른분들이 쓰신 [0, None, 0, 0, 0, None, None, None, None, None, 1, None, None, 1, None, None, None, 0, 0, 1, None, 0, None, 1, 1, None, 1, None, None, None, None, 1, None, None, 1, None, None, None, 1, 1, None, None, 1, None, 0, None, None, 0, 1, None, None, None, 1, None, None, None, None, 1, None, 1]\n"
     ]
    }
   ],
   "source": [
    "sent_corrected, tags = model.correct('기존매트리스오래되어서 버리고 설치했어요. 프레임에 딱 맞게 들어가구요 타입은 약간폭신 과 하드한 타입이있는데 구매평에 다른분들이 쓰신')\n",
    "print(sent_corrected, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "training ... (0 in 1831 sents) use memory 0.185 Gb\r",
      "training was done. used memory 0.184 Gb\n",
      "\r",
      " cohesion probabilities ... (1 in 1534)\r",
      "all cohesion probabilities was computed. # words = 1126\n",
      "\r",
      "all branching entropies was computed # words = 2140\n",
      "\r",
      "all accessor variety was computed # words = 2140\n",
      "1283\n"
     ]
    }
   ],
   "source": [
    "# word extractor\n",
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(sents)\n",
    "words = word_extractor.extract()\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=1260, neg=1173, common=12\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6412 from 1831 sents. mem=0.101 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10922, mem=0.115 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2497 words\n",
      "[Noun Extractor] checked compounds. discovered 145 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 642 -> 630\n",
      "[Noun Extractor] postprocessing ignore_features : 630 -> 620\n",
      "[Noun Extractor] postprocessing ignore_NJ : 620 -> 618\n",
      "[Noun Extractor] 618 nouns (145 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.125 Gb                    \n",
      "[Noun Extractor] 40.80 % eojeols are covered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('정말포근', ('정말', '포근')),\n",
       " ('베개커버', ('베개', '커버')),\n",
       " ('매트리스후기', ('매트리스', '후기')),\n",
       " ('해피주말', ('해피', '주말')),\n",
       " ('삼분의일체험', ('삼분의일', '체험'))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noun extractor\n",
    "noun_extractor = LRNounExtractor_v2(verbose=True)\n",
    "nouns = noun_extractor.train_extract(list(sents))\n",
    "list(noun_extractor._compounds_components.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-078f8a521aa4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mokt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOkt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sents' is not defined"
     ]
    }
   ],
   "source": [
    "# 명사 추출 konlpy\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "for i, sent in enumerate(sents) :\n",
    "    if i > 10 : break\n",
    "    print(sent)    \n",
    "    print(okt.nouns(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
